{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Beat It!.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CllhDUEvjJN8"},"source":["Google Colab notebook used in the final year project Beat It!\n","\n","Code written by Matthew Dwyer 17330141.\n","\n","Mount dataset from Google Drive using codeblock two using the credentials provided in README.md on GitLab\n","\n","Directory variables are subject to change depending on where the dataset is saved."]},{"cell_type":"code","metadata":{"id":"XrZ9PyPa5Tyi"},"source":["#Install necessary packages\n","!pip install tensorflow torch torchaudio "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lW2k5EgxY4Yj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620241185268,"user_tz":-60,"elapsed":19464,"user":{"displayName":"Matthew Dwyer","photoUrl":"","userId":"14200494475002814317"}},"outputId":"c0bd7014-fc7e-4abc-aee3-48e26ca56108"},"source":["#Mount to google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QMD_yiRitGVx","executionInfo":{"status":"ok","timestamp":1620241273345,"user_tz":-60,"elapsed":6238,"user":{"displayName":"Matthew Dwyer","photoUrl":"","userId":"14200494475002814317"}}},"source":["#Import necessary packages\n","import torchaudio\n","import torch\n","import torchaudio.transforms as T\n","\n","import tensorflow as tf\n","tf.config.run_functions_eagerly(True)\n","\n","import matplotlib.pyplot as plt\n","import os\n","import glob\n","import numpy as np\n","from sklearn import model_selection as ms\n","from tqdm import tqdm"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuOAW-dlQdzV","executionInfo":{"status":"ok","timestamp":1620241274189,"user_tz":-60,"elapsed":838,"user":{"displayName":"Matthew Dwyer","photoUrl":"","userId":"14200494475002814317"}}},"source":["#Setting random seeds to increase reproducibility\n","os.environ['PYTHONHASHSEED'] = '0'\n","np.random.seed(1)\n","tf.random.set_seed(1)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWN02qgTtGg1"},"source":["#Connect to gpu\n","print(torch.cuda.device_count())\n","print(torch.cuda.get_device_name(0))\n","device = torch.device(\"cuda:0\")\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1uze00HQ0Kl","executionInfo":{"status":"ok","timestamp":1620241274190,"user_tz":-60,"elapsed":836,"user":{"displayName":"Matthew Dwyer","photoUrl":"","userId":"14200494475002814317"}}},"source":["def buildData(n_fft, slice_len,rebuild=False):\n","    #Define directoies needed for data preprocessing\n","    audio_directory = \"/content/drive/My Drive/wavs/\"\n","    annotation_directory = \"/content/drive/My Drive/BPMs/\"\n","    spectrogram_directory = \"/content/drive/My Drive/data/spectrogram_pts/\"\n","    tensor_directory = \"/content/drive/My Drive/data/tensor_pts/\"\n","    if rebuild:\n","        #First delete all the files previously created\n","        spec_fnames = list(fname for fname in os.listdir(spectrogram_directory) if fname.endswith('.pt'))\n","        tensor_fnames = list(fname for fname in os.listdir(tensor_directory) if fname.endswith('.pt'))        \n","        for f in spec_fnames:\n","          os.remove(spectrogram_directory+f)\n","        for f in tensor_fnames:\n","          os.remove(tensor_directory+f)\n","        #Define values needed for mel spectrogram computation\n","        n_fft = n_fft\n","        win_length = None\n","        hop_length = 512\n","        n_mels = 40\n","        fixed_sample_rate = 11025\n","\n","        #Grab all file names from dataset\n","        fnames = list(fname for fname in os.listdir(audio_directory) if fname.endswith('.wav'))\n","\n","        for fname in tqdm(fnames):\n","            #List of mel spectrograms for this file \n","            melspecs = []\n","            #List of bpm tensors for this file\n","            bpm_tensors = []\n","            bpm_file = fname[:-3]\n","            bpm_file = bpm_file+\"txt\"\n","\n","            #Get bpm value from file\n","            f = open(annotation_directory+bpm_file)\n","\n","            file_name = fname[:-3]\n","            file_name = file_name+\"pt\"\n","\n","            bpm = f.read() \n","            bpm = float(bpm)\n","            bpm = int(bpm)\n","            index = bpm-30\n","\n","            #Load the wav file\n","            waveform, sample_rate = torchaudio.load(audio_directory+fname)\n","\n","            #Transform audio sample to mono\n","            resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=fixed_sample_rate)\n","            audio_mono = torch.mean(resample_transform(waveform), dim=0, keepdim=True)\n","\n","            mel_spectrogram = T.MelSpectrogram(\n","              sample_rate=fixed_sample_rate,\n","              n_fft=n_fft,\n","              win_length=win_length,\n","              hop_length=hop_length,\n","              n_mels=n_mels,\n","            )\n","\n","            #compute mel spectrogram of mono audio\n","            melspec = mel_spectrogram(audio_mono)\n","\n","            #Slice the mel spectrogram for every 256 units of length, step of 128\n","            #Half overlapping windows\n","            for lbound in range(0,melspec.shape[2],int(slice_len/2)):\n","                ubound = lbound+slice_len\n","                if ubound > melspec.shape[2]:\n","                    spec_temp = melspec[:,:,lbound:]\n","                    padding = slice_len-spec_temp.shape[2]\n","                    zeropad = torch.nn.ZeroPad2d((0,padding,0,0))\n","                    spec_temp = zeropad(spec_temp)\n","                    melspecs.append(spec_temp)\n","\n","                    tensor = np.zeros(256)\n","                    tensor[index-1] = 1\n","                    bpm_temp = torch.from_numpy(tensor)\n","                    bpm_tensors.append(bpm_temp)\n","                else:\n","                    spec_temp = melspec[:,:,lbound:ubound]\n","                    melspecs.append(spec_temp)\n","\n","                    tensor = np.zeros(256)\n","                    tensor[index-1] = 1\n","                    bpm_temp = torch.from_numpy(tensor)\n","                    bpm_tensors.append(bpm_temp)\n","\n","            #Save the mel spectrogram and bpm tensors    \n","            melspec_tensor = torch.stack(melspecs)\n","            torch.save(melspec_tensor, spectrogram_directory+file_name)\n","\n","            bpm_tensor = torch.stack(bpm_tensors)\n","            torch.save(bpm_tensor, tensor_directory+file_name)\n","            \n","    #Grab file names of files\n","    spec_fnames = list(fname for fname in os.listdir(spectrogram_directory) if fname.endswith('.pt'))\n","    tensor_fnames = list(fname for fname in os.listdir(tensor_directory) if fname.endswith('.pt'))\n","    \n","    X = torch.load(spectrogram_directory+spec_fnames[0]).view(-1,1,40,slice_len)\n","    y = torch.load(tensor_directory+tensor_fnames[0])\n","    #Delete these elements from the training set\n","    del spec_fnames[0]\n","    del tensor_fnames[0]\n","    #Loop over the remaining examples and labels in the \n","    #training set and concatenate them to X and y\n","    for x in tqdm(spec_fnames):\n","        temp_spec = torch.load(spectrogram_directory+x).view(-1,1,40,slice_len)\n","        X = torch.cat((X,temp_spec),0)\n","\n","        temp_tensor = torch.load(tensor_directory+x)\n","        y = torch.cat((y,temp_tensor),0)\n","\n","        if temp_spec.shape[0] != temp_tensor.shape[0]:\n","          print(x)\n","        \n","    X_train, X_test, y_train, y_test = ms.train_test_split(X.numpy(), y.numpy(), test_size=0.2, random_state=1)\n","    X_train,X_val, y_train, y_val = ms.train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n","    del X\n","    del y\n","    \n","    X_train = tf.reshape(tf.convert_to_tensor(X_train),(-1,40,slice_len,1))\n","    y_train = tf.reshape(tf.convert_to_tensor(y_train),(-1,256))\n","    \n","    X_val = tf.reshape(tf.convert_to_tensor(X_val),(-1,40,slice_len,1))\n","    y_val = tf.reshape(tf.convert_to_tensor(y_val),(-1,256))\n","    \n","    X_test = tf.reshape(tf.convert_to_tensor(X_test),(-1,40,slice_len,1))\n","    y_test = tf.reshape(tf.convert_to_tensor(y_test),(-1,256))\n","\n","    print(\"Training set has \"+str(X_train.shape[0])+\" samples and labels\")\n","    print(\"Testing set has \"+str(X_test.shape[0])+\" samples and labels\")\n","    print(\"Validation set has \"+str(X_val.shape[0])+\" samples and labels\")\n","\n","    return X_train,y_train,X_val,y_val,X_test,y_test"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0hUVReERVi7"},"source":["class TFInception(tf.keras.layers.Layer):\n","    def __init__(self, n_filters=24, kernel_sizes=[32, 64, 96, 128, 192, 256], bottleneck_channels=36):\n","        super(TFInception, self).__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(1,kernel_sizes[0]), padding=\"same\", activation='elu')\n","        self.conv2 = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(1,kernel_sizes[1]), padding=\"same\", activation='elu')\n","        self.conv3 = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(1,kernel_sizes[2]), padding=\"same\", activation='elu')\n","        self.conv4 = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(1,kernel_sizes[3]), padding=\"same\", activation='elu')\n","        self.conv5 = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(1,kernel_sizes[4]), padding=\"same\", activation='elu')\n","        self.conv6 = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(1,kernel_sizes[5]), padding=\"same\", activation='elu')\n","        self.bottleneck = tf.keras.layers.Conv2D(filters=bottleneck_channels, kernel_size=(1,1), activation='elu')\n","\n","    def call(self, x):\n","        a = tf.concat([self.conv1(x), self.conv2(x), self.conv3(x), self.conv4(x), self.conv5(x), self.conv6(x)], axis=3)\n","        return self.bottleneck(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rmDg1gnRVal"},"source":["with tf.device('/CPU:0'): \n","  TFModel = tf.keras.Sequential(\n","  [\n","      #Input layer\n","      tf.keras.layers.InputLayer(input_shape=(40,128,1)),\n","   \n","      #Short Conv Layers\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Conv2D(filters=16, kernel_size=(1,5), padding=\"same\", activation = \"elu\"),\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Conv2D(filters=16, kernel_size=(1,5), padding=\"same\", activation = \"elu\"),\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Conv2D(filters=16, kernel_size=(1,5), padding=\"same\", activation = \"elu\"),\n","\n","      #Inception Layers\n","      tf.keras.layers.AveragePooling2D(pool_size=(5,1), strides=(5,1)),\n","      tf.keras.layers.BatchNormalization(),\n","      TFInception(),\n","      tf.keras.layers.AveragePooling2D(pool_size=(2,1), strides=(2,1)),\n","      tf.keras.layers.BatchNormalization(),\n","      TFInception(),\n","      tf.keras.layers.AveragePooling2D(pool_size=(2,1), strides=(2,1)),\n","      tf.keras.layers.BatchNormalization(),\n","      TFInception(),\n","      tf.keras.layers.AveragePooling2D(pool_size=(2,1), strides=(2,1)),\n","      tf.keras.layers.BatchNormalization(),\n","      TFInception(),\n","      \n","      #Linear Layers\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dropout(rate=0.5),\n","      tf.keras.layers.Dense(128),\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Dense(64),\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Dense(256,activation=\"softmax\"),\n","  ]\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Peq4UkYRdzl"},"source":["#Check the architecture of the model is correct\n","TFModel.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3tEjFj_mEgY"},"source":["#Define accuracy 1, accuracy within 4% margin of error\n","def accuracy1(y_true, y_pred):\n","  total = 0\n","  correct = 0\n","  for true, pred in zip(y_true, y_pred):\n","    true_bpm = tf.math.argmax(true)\n","    true_bpm = true_bpm+30\n","    margin_of_error = (np.array(true_bpm)*0.04)\n","    ubound = true_bpm + margin_of_error\n","    lbound = true_bpm - margin_of_error\n","    pred_bpm = tf.math.argmax(pred)\n","    pred_bpm = pred_bpm+30\n","    total+=1\n","    if lbound <= pred_bpm <= ubound:\n","      correct+=1\n","  accuracy1 = correct/total\n","  return(accuracy1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKAzqVsrI-QR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620242764832,"user_tz":-60,"elapsed":1490429,"user":{"displayName":"Matthew Dwyer","photoUrl":"","userId":"14200494475002814317"}},"outputId":"3a1e9c5a-1f26-44e6-fbed-2c9528b57cf4"},"source":["X_train,y_train,X_val,y_val,X_test,y_test = buildData(n_fft=1024,slice_len=256,rebuild=True)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["100%|██████████| 882/882 [19:34<00:00,  1.33s/it]\n","100%|██████████| 881/881 [05:08<00:00,  2.85it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Training set has 9591 samples and labels\n","Testing set has 3197 samples and labels\n","Validation set has 3197 samples and labels\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-Q77Gfyq13ve"},"source":["#Define the optimizer and compile the network\n","opt = tf.keras.optimizers.Adam(learning_rate=0.0001,epsilon=1e-8)\n","TFModel.compile(optimizer=opt,loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy',accuracy1])\n","\n","#Define where to save the checkpoint with the best validation accuracy\n","checkpoint_directory = \"/content/drive/My Drive/cnn_best_val.h5\"\n","\n","#Define callback to save checkpoint with best validaiton accuracy\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_directory,\n","    save_weights_only=True,\n","    monitor='val_loss',\n","    mode='min',\n","    save_best_only=True)\n","\n","#Define callback to stop training when the validaiton loss\n","#does not improve over 10 epochs\n","early_stop_callback = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', \n","    patience=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPbpk3CCmnCB"},"source":["#Fit the network to the training data\n","history = TFModel.fit(X_train,y_train, epochs = 100, batch_size=32, validation_data=(X_val, y_val), \n","                  validation_batch_size=32, callbacks=[early_stop_callback,model_checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OimwE8qKXQXi"},"source":["#Create graph for training and validation accuracy0 scores\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n","plt.axvline(x = len(history.history['accuracy'])-11, label = \"Lowest Validation Checkpoint\",linestyle='--',color=\"red\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0, 1])\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSWICK5lQ5Al"},"source":["#Create graph for training and validation accuracy1 scores\n","plt.plot(history.history['accuracy1'], label='Training Accuracy 1')\n","plt.plot(history.history['val_accuracy1'], label = 'Validation Accuracy 1')\n","plt.axvline(x = len(history.history['accuracy'])-11, label = \"Lowest Validation Checkpoint\",linestyle='--',color=\"red\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy 1')\n","plt.ylim([0.4, 1])\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy 1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0u3n0wcRbmmm"},"source":["#Create graph for training and validation loss\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label = 'Validation Loss')\n","plt.axvline(x = len(history.history['accuracy'])-11, label = \"Lowest Validation Checkpoint\",linestyle='--',color=\"red\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgCcWZhQXRaW"},"source":["#Load the model weights with the lowest validation loss\n","#and test it on the testing set\n","TFModel.load_weights(checkpoint_directory)\n","test_loss, test_acc, test_acc1 = TFModel.evaluate(X_test,  y_test)"],"execution_count":null,"outputs":[]}]}